---
title: Using the @telemetry Directive
description: "Learn how to configure observability support using OpenTelemetry for insights into logs, metrics, and traces. Discover practical integration examples for platforms like Honeycomb.io, New Relic, and Datadog."
slug: graphql-telemetry-guide
sidebar_label: Telemetry
---

Observability is a critical aspect of maintaining and optimizing modern applications. In this guide, we will explore how to enable and configure observability in Tailcall, focusing on the collection and analysis of telemetry data using different observability backends. By the end of this guide, you will learn how to:

- Enable telemetry data generation in Tailcall.
- Configure Tailcall to forward telemetry data to various observability platforms.
- Integrate Tailcall with popular observability tools using real-world examples.

Let’s get started!

## What is Observability

Observability is essential for maintaining the health and performance of your applications. It provides insights into your software's operation in real-time by analyzing telemetry data — logs, metrics, and traces. This data helps in troubleshooting, optimizing, and ensuring your application works as expected.

- **Logs** offer a record of events that have happened within your application, useful for understanding actions taken or errors that have occurred.
- **Metrics** are numerical data that measure different aspects of your system's performance, such as request rates or memory usage.
- **Traces** show the journey of requests through your system, highlighting how different parts of your application interact and perform.

### OpenTelemetry

Tailcall integrates with the [OpenTelemetry](https://opentelemetry.io) specification, a standardized toolkit for collecting telemetry data across different platforms and languages. OpenTelemetry allows you to avoid being locked into a single observability platform, enabling you to send your data to various tools for analysis, such as New Relic or Honeycomb.

**Screenshot Suggestion**: Include a diagram showing the flow of telemetry data from Tailcall to different observability platforms using OpenTelemetry.

## Comparison with Apollo Studio

While [Apollo Studio](./apollo-studio.md) offers telemetry and analytics tools specifically for GraphQL schemas, there are key differences when compared to OpenTelemetry integration in Tailcall:

- **Generalized vs. Specific Insights**: OpenTelemetry provides generalized observability capabilities that can be applied across multiple services, whereas Apollo Studio focuses solely on GraphQL insights.
- **Vendor-Agnostic Flexibility**: OpenTelemetry is vendor-agnostic, allowing you to choose different observability platforms as needed, unlike Apollo Studio, which is tied to its own ecosystem.
- **Broader Analytical Scope**: Tailcall’s integration with OpenTelemetry allows for more comprehensive analytical data, extending beyond the scope of GraphQL-specific metrics provided by Apollo Studio.

## Prerequisites

Consider we have the following GraphQL configuration that connects with jsonplaceholder.com to fetch the data about user and posts

```graphql
schema
  @server(port: 8000, hostname: "0.0.0.0")
  @upstream(
    baseURL: "http://jsonplaceholder.typicode.com"
  ) {
  query: Query
}

type Query {
  posts: [Post] @http(path: "/posts") @cache(maxAge: 3000)
  user(id: Int!): User @http(path: "/users/{{.args.id}}")
}

type User {
  id: Int!
  name: String!
  username: String!
  email: String!
  phone: String
  website: String
}

type Post {
  id: Int!
  userId: Int!
  title: String!
  body: String!
  user: User @http(path: "/users/{{.value.userId}}")
}
```

We will update that config with telemetry integration in following sections.

## GraphQL Configuration for Telemetry

By default, telemetry data is not generated by Tailcall since it requires setup to know where to send the data. It also affects performance of the server that could be undesirable in some cases.

Telemetry configuration is provided by [`@telemetry`](/docs/directives.md#telemetry-directive) directive to setup how and where the telemetry data is sent.

To enable it, we can update our config with something like the config below:

```graphql
schema
  @telemetry(
    export: {
      otlp: {url: "http://your-otlp-compatible-backend.com"}
    }
  ) {
  query: Query
}
```

In this configuration:

- The `export` option specifies the format and endpoint where the telemetry data will be sent.
- Replace `http://your-otlp-compatible-backend.com` with the URL of your observability platform that supports OTLP.

## Exporting Telemetry Data

### Export to OTLP

#### What is OTLP?

[OTLP (OpenTelemetry Protocol)](https://opentelemetry.io/docs/specs/otlp/) is a vendor-agnostic standard for exporting telemetry data. It is widely supported by a growing [number of observability backends](https://opentelemetry.io/ecosystem/vendors/).

#### Using OpenTelemetry Collector

[OpenTelemetry Collector](https://opentelemetry.io/docs/collector/) is a robust solution for receiving, processing, and exporting telemetry data in OTLP format. While Tailcall can directly send data to OTLP-compatible platforms, using the OpenTelemetry Collector provides additional benefits:

- **Scalability**: The Collector is designed to handle high loads and complex setups.
- **Flexibility**: It can export data in multiple formats, such as Jaeger or Datadog, and is well-suited for large-scale environments.

**Configuration Example**:

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

exporters:
  otlp:
    endpoint: otelcol:4317

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
```

### Export to Prometheus

Prometheus is a popular open-source monitoring solution focused on metrics. It is well-suited for applications that require detailed metrics monitoring.

To export metrics to Prometheus, Tailcall needs to expose metrics in a format that Prometheus can scrape. This is typically done by adding a special route to the GraphQL server:

```graphql
schema
  @telemetry(export: {prometheus: {path: "/metrics"}}) {
  query: Query
}
```

### Export to stdout

Tailcall can also output telemetry data to stdout, which is ideal for testing or local development environments.

```graphql
schema @telemetry(export: {stdout: {pretty: true}}) {
  query: Query
}
```

## Using Telemetry Data

### Data Generated

Tailcall generates various types of telemetry data, including:

- **Metrics**: Such as request counts, error rates, and latencies.
- **Traces**: Showing the flow of requests across different services.
- **Logs**: Detailed event logs for specific actions or errors.

### Context Propagation and Distributed Tracing

Tailcall fully supports [context propagation](https://opentelemetry.io/docs/concepts/context-propagation/) functionality. Context propagation allows you to track distributed traces across multiple services, providing a comprehensive view of how a request flows through your system.

Here's an example of using context propagation with Honeycomb:

![honeycomb-propagation](../static/images/telemetry/honeycomb-propagation.png)

### Customization

In some cases you may want to customize the data that was added to telemetry payload in order to have more control over the analyzing process. Tailcall allows you to customize metrics by using properties like [`requestHeaders`](/docs/directives.md#requestheaders), which can be used to segment data by specific headers.

**Example**:

```graphql
schema @telemetry(requestHeaders: ["X-User-Id"]) {
  query: Query
}
```

:::important
The value of specified headers will be sent to the telemetry backend as is. Be cautious when including sensitive information in telemetry data to avoid unintentional data leaks.
:::

## Troubleshooting and Debugging Telemetry Issues

Despite the robust telemetry capabilities in Tailcall, you may encounter issues that require troubleshooting and debugging. This section provides guidance on how to diagnose and resolve common telemetry problems.

### Common Telemetry Issues

1. **No Telemetry Data is Being Collected**

   - **Check Configuration**: Ensure that the `@telemetry` directive is correctly configured in your GraphQL schema. Verify that the export endpoints are correctly specified and reachable.
   - **Network Connectivity**: Confirm that there is network connectivity between your Tailcall server and the observability backend. Check firewall rules, DNS settings, and endpoint URLs.
   - **Telemetry Data Volume**: If the telemetry data volume is too low, you may not see data immediately. Generate additional traffic to the application to verify data collection.

2. **Incomplete or Missing Traces**

   - **Context Propagation Issues**: Verify that context propagation is correctly configured. In distributed systems, missing traces often result from improper context propagation across services.
   - **Instrumentation Gaps**: Ensure all necessary services and components are properly instrumented with OpenTelemetry. Missing or incomplete instrumentation can lead to gaps in tracing data.

3. **High Latency or Performance Degradation**

   - **Resource Overhead**: Telemetry collection can introduce overhead, especially if you are exporting a large volume of data. Consider optimizing the frequency of data collection, reducing the amount of data exported, or using more efficient telemetry backends.
   - **Collector Bottlenecks**: If using OpenTelemetry Collector, monitor its performance to ensure it is not a bottleneck. Adjust configuration settings or scale the Collector to handle larger volumes of telemetry data.

4. **Incorrect Metrics or Logs**
   - **Validation of Telemetry Data**: Compare the telemetry data against expected values. Discrepancies could be due to misconfiguration of metric instruments or log formats.
   - **Export Configuration**: Ensure that the export configuration, such as Prometheus scraping paths or OTLP endpoints, matches the telemetry backend's requirements. Misconfigured paths or formats can lead to incorrect data being collected.

### Debugging Steps

1. **Enable Debug Logging**

   - OpenTelemetry has a debug logging mode that can be enabled to provide more detailed output. Use these logs to identify where issues are occurring in the telemetry pipeline.

2. **Use Local Development Tools**

   - For quick testing, configure Tailcall to export telemetry data to `stdout`.

3. **Validate Export Endpoints**

   - Test the telemetry export endpoints directly (e.g., using `curl` for HTTP endpoints) to ensure they are reachable and correctly configured. This helps rule out network issues or misconfigured endpoints.

4. **Check Compatibility with Observability Tools**

   - Ensure that the observability tool you are using is fully compatible with the telemetry format being exported. Refer to the observability tool's documentation to confirm compatibility with OTLP, Prometheus, or other formats you are using.

5. **Monitor Resource Usage**
   - Use system monitoring tools to observe resource usage by the OpenTelemetry Collector. High CPU, memory, or network usage could indicate inefficiencies in your telemetry setup that need to be addressed.

### Advanced Debugging

If the above steps do not resolve the issue, consider using more advanced debugging techniques:

- **Trace Logs**: Enable trace-level logging in OpenTelemetry to get detailed logs of each step in the telemetry data flow.
- **Distributed Tracing**: Use distributed tracing to follow the telemetry data as it passes through different services and components in your architecture. This can help identify where data is lost or delayed.
- **Profiling**: Use performance profiling tools to identify bottlenecks in your telemetry pipeline, particularly if you suspect high overhead or resource contention.

By following these troubleshooting and debugging strategies, you can resolve common telemetry issues in Tailcall and ensure that your observability setup functions optimally.

## Conclusion

In this guide, we have covered the essentials of enabling and configuring observability in Tailcall. You should now be able to:

- Generate telemetry data and forward it to your preferred observability platforms.
- Customize telemetry configurations to meet specific needs.
- Troubleshoot common issues related to telemetry in Tailcall.

As a next step, we encourage you to experiment with these features in your own projects and explore further integration with advanced observability tools. Happy monitoring!
